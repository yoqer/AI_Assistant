services:
  python-genai:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8081:8081"
    environment:
      - PORT=8081
      - LOG_LEVEL=INFO
      - LLM_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
      - LLM_MODEL_NAME=ai/qwen2.5:latest
      - OPENROUTER_API_KEY
#    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    volumes:
      - ./:/app
    depends_on:
      - llm

  llm:
    provider:
      type: model
      options:
        model: ai/qwen2.5:latest
