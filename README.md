# ğŸ§  Simple AI Assistant Docker App that Remembers

This project is a **production-ready AI chat app** built with Docker Model Runner, Streamlit, and LangChain. It lets you talk to a local LLM running via Docker, or switch seamlessly to a large cloud-based model (like those on OpenRouter), all **while remembering your entire conversation history**. Each model will get the full chat context, even in moments when it wasn't participating in the conversation.

## ğŸ¥ Video Tutorial

To build this application step by step, please watch my YouTube tutorial below. It will explain all the app processes, and the logic behind them.

<a href="https://youtu.be/oIqF0z2UhDM" target="_blank"><img width="600" alt="LLM Chat App with Memory thumbnail" src="https://github.com/user-attachments/assets/212c47bc-de25-4e35-9c23-e747a33d5d6e" /></a>

## â­ Features

1. Run local open-source LLMs with Docker Model Runner ğŸ¤–
2. Clean Streamlit chat interface with message history ğŸ’»
3. Seamless switch between local and cloud models ğŸ•¹ï¸
4. Context-passing for memory-aware responses ğŸ’¡
5. Fully containerized with Docker Compose  ğŸ‹


## ğŸ“¸ Screenshot

<img width="600" alt="screenshot of Simple AI Assistant Chat App" src="https://github.com/user-attachments/assets/789ec4fd-bf47-44e7-ad3a-052dc954582a" />

## âš¡ï¸ Quick Start

### 1ï¸âƒ£ Prerequisites

- [Docker Desktop](https://www.docker.com) installed and updated to current version.
- Docker Model Runner **enabled** in Docker Desktop (see [official docs](https://dockr.ly/4nT2saM))

### 2ï¸âƒ£ Clone This Repo

<pre>
git clone https://github.com/MariyaSha/simple_AI_assistant.git
simple_AI_assistant
</pre>

### 3ï¸âƒ£ Create a `.env` File

Create a file named `.env` in the project root:

<pre>
LOCAL_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
REMOTE_BASE_URL=https://openrouter.ai/api/v1
LOCAL_MODEL_NAME=ai/gemma3
REMOTE_MODEL_NAME=qwen/qwen3-30b-a3b
OPENROUTER_API_KEY=YOUR_OPENROUTER_API_KEY
</pre>

- Replace `YOUR_OPENROUTER_API_KEY` with your actual OpenRouter key.  
- Choose any local or remote model from:
  - ğŸ‘‰ [Docker Model Catalog](https://dockr.ly/4eTeLQl)
  - ğŸ‘‰ [OpenRouter](https://openrouter.ai)

### 4ï¸âƒ£ Run the App

<pre>
docker compose up
</pre>

Then open your browser to:

http://localhost:8501  

âœ… Your AI chat app is ready to use!

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ app.py                # Streamlit chat app with LangChain
â”œâ”€â”€ Dockerfile            # Container for running the app
â”œâ”€â”€ docker-compose.yaml   # Defines app + model services
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ .env                  # Environment variables (you create this)
```


## ğŸ§© How It Works

- The **llm** service in `docker-compose.yaml` uses Docker Model Runner to serve a local LLM (e.g. Gemma).
- The **ai-app** service is a Streamlit web app that:
  - Stores message history in session state
  - Passes the entire chat context to the LLM for memory-aware responses
  - Lets you switch between local and remote models with a simple checkbox


## ğŸš€ Customization

- **Change Local Model**  
  Edit `LOCAL_MODEL_NAME` and `LOCAL_BASE_URL` in your `.env`.

- **Change Remote Model**  
  Edit `REMOTE_MODEL_NAME`, `REMOTE_BASE_URL`, and your `OPENROUTER_API_KEY`.

- **Dependencies**  
  Add any extra Python packages to `requirements.txt`.


## ğŸ“š Helpful Links

- ğŸ³ [Docker Model Runner Documentation](https://dockr.ly/4nT2saM)  
- ğŸ” [Find Models in Docker Catalog](https://dockr.ly/4eTeLQl)  
- ğŸŒ [OpenRouter](https://openrouter.ai)  
- ğŸ [Streamlit](https://streamlit.io)  
- ğŸ¦œ [LangChain for Python](https://python.langchain.com)


## ğŸ¤ Contributing

If you'd like to contribute, please create an issue and describe what you have in mind.
<br>
I'm trying to keep this repository as close as possible to the video workflow, but if you'd like to take it to the next level, I can split it in two.
